---
title: "Multivariate Analysis: First Assignment"
author: "Marc Pastor Pou, Juan Ángel Pérez Córcoles, Eduardo Gambín Monserrat"
date: "`r Sys.Date()`"
tbl-cap-location: bottom
output:
  html_document:
    self_contained: true
    keep_md: false
    css: styles.css
format:
  html:
    toc: true
    toc-title: "Table of Contents"
    toc-depth: 3 # Controls heading levels (e.g., H1, H2, H3)
    toc-location: left # You can also use "right"
    number-sections: false # Adds numbering to headings
    smooth-scroll: true
  pdf:
    documentclass: scrartcl
    papersize: a4paper
    geometry: margin=25mm
    monofont: "JetBrains Mono"
    fig-pos: "!htp"
    toc: false
    number-sections: true
    keep-tex: false
    fig-width: 6
    fig-height: 4
    fontsize: 10pt
    pdf-engine: xelatex
    include-in-header: preamble.tex
    df-print: default
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = FALSE
)
```

```{r}
pacman::p_load(
tidyverse,
DataExplorer,
labelled,
janitor,
gtsummary,
gt,
corrplot,
ggcorrplot,
GGally,
FactoMineR,
factoextra,
leaflet,
pheatmap,
sf,
cluster,
dbrobust,
DescTools,
vegan,
biotools,
fpc
)
load("../BBDD/resultados_airbnb.RData")
```

## Introduction

For this second assignment we continue to work with the data from [Inside Airbnb](https://insideairbnb.com/) database. This time we compute a two Multidimensional Scaling (MDS) techniques; MDS based on G-Gower distance and Related Metric Scaling (RelMS) and a Cluster analysis.

## MDS

In this assignment we compute an MDS based on G-Gower Distance and an MDS configuration called Related Metric Scaling (RelMS). G-Gower distance is a generalization of Gower Distance which helps us compute dissimilarities between mixed data types, allowing us to observe the overall similarity across all variables, giving a broad view of how items relate. RelMS emphasizes certain prespecified relationships or structures in the data while still preserving distances, RelMS highlights specific structures or patterns in the data .

### G-Gower and RelMS comparison

#### Variance Comparison

The @fig-plot-variance-comparison shows how much variation is captured by a specific dimension for each method. As we can observe for the first two dimensions G-Gower is able to capture more variation while for the rest of dimensions RelMS is able to capture a little bit more variation. This is because G-Gower is able to capture the more "general" view of the patterns in our data while RelMS capture the more "subtle" details that appear in later dimensions.

```{r}
#| label: fig-plot-variance-comparison
#| fig-cap: "Variance comparison for G-Gower and RelMs."
plot_variance_comparison
```

#### Cumulative Variance Comparison

Another way we can observe the performance of both methods in capturing the variation of each dimension is through the @fig-plot-cumulative-variance-comparison, which shows the cumulative variance. Even though both models capture a significant amount of cumulative variation (both beyond the 70% threshold), the G-Gower is able to capture a little bit more of cumulative variation than the RelMS.

```{r}
#| label: fig-plot-cumulative-variance-comparison
#| fig-cap: "Cumulative Variance comparison for G-Gower and RelMs."
plot_cumulative_comparison
```

### G-Gower and RelMS correlation with original variables

The following plots allows us to see the correlation between each variable and each MDS dimension computed by both methods.

#### Correlation Heatmap for G-Gower

We first look at our heatmap for G-Gower @fig-heatmap-associations_ggower. The first dimension here separates high-value listings with full amenities from more basic options this is clear because it has a high positive correlation with the variables like: Price, Accommodates, Air Conditioning and Heating. The second dimension presents a higher positive correlation with the variables: Elevator and Air Conditioning and a negative correlation with the Number of Reviews, which are more like secondary features when looking for a house. The third dimension has a high positive correlation with Heating, the rest of the dimensions start showing less correlation between the variables with the dimensions because of the G-Gower distance tendency to give "general" explanations.

```{r}
#| label: fig-heatmap-associations_ggower
#| fig-cap: "Correlation Heatmap for G-Gower."
heatmap_associations_ggower
```

We now look at our heatmap for RelMS @fig-heatmap-associations_relms. The first dimension is similar to the one for GG-power but in this case it gives a higher correlation value to the variable Air Conditioning. The same can be said for the second dimension in which RelMS captures a high correlation for the same variables as G-Gower but in this case it gives a higher correlation value to Elevator. For the next dimensions it captures a bigger amount of correlation between the dimensions and the variables because of its nature to give more specific insights in the data.

#### Correlation Heatmap for RelMS

```{r}
#| label: fig-heatmap-associations_relms
#| fig-cap: "Correlation Heatmap for RelMS."
heatmap_associations_relms
```

### Comparison plots for categorical variables

The following section we compute a series of MDS comparison plots between dimension 1 and dimension 2 for a series of variables.

#### Comparison plot for `room_type`

The following plot @fig-plot-comparison-room_type, allows us to observe how each type of room correlates with each dimension. But the most interesting part of this plot is that it shows that the G-Gower method points are more separated between them showing its "generalization" nature while the points in RelMS are more compacted providing us a look into its specific nature. A better way this plot shows us the difference nature of both models in when we look at the hotel room data point, which is clearly seen in the RelMS section while in the G-Gower section is hidden behind other point.

```{r}
#| label: fig-plot-comparison-room_type
#| fig-cap: "Comparison plot for room type."
create_comparison_plots("room_type")
```

#### Comparison plot for `neighbourhood_group_cleansed`

The following plot @fig-plot-comparison-neighborhood-group, allows us to observe how each neighborhood correlates with each dimension. Here again we see the nature of each model and how G-Gower is able to capture more variation than the RelMS.

```{r}
#| label: fig-plot-comparison-neighborhood-group
#| fig-cap: "Comparison plot for neighborhood group."
create_comparison_plots("neighbourhood_group_cleansed")
```

#### Comparison plot for `neighbourhood_group_cleansed` (Dimensions 1 and 5)

This plot @fig-plot-comparison-neighborhood-group-d1-d5, has the same purpose as the previous one, the main difference is that now we compare dimensions 1 and 5. We can now see that for lower dimensions RelMS capture a little bit more of variation which affects the correlation between the variables and the dimensions as we have seen in the previous heatmaps.

```{r}
#| label: fig-plot-comparison-neighborhood-group-d1-d5
#| fig-cap: "Comparison plot for neighborhood group(dimensions 1 and 5)."
create_comparison_plots("neighbourhood_group_cleansed", dim_x = 1, dim_y = 5)
```

### Bootstrap stability analysis

As we did in our previous assignment, we compute a bootstrap stability analysis for our MDS methods.

#### Eigenvalue stability plot for G-Gower and RelMS

In this figure @fig-plot-eigenvalue-stability we can see the same results as we have seen before. While G-Gower captures more variation for the the first two dimensions, showing its efficiency in "packing" the general parts of the data, RelMS is able to capture more variation in later dimensions which shows its capability in finding meaningful structures in the data.

```{r}
#| label: fig-plot-eigenvalue-stability
#| fig-cap: "Eigenvalue stability plot for G-Gower and RelMS."
plot_eigen_stability
```

### Coefficient of variation plot for G-Gower and RelMS

This plot @fig-plot-coefficient-of-variation-comparison shows us the reliability of each of the methods. Even though G-Gower captures more variation for the first two dimensions, RelMS is more robust. The increase of the coefficient of variation is due to the fact that as we get further from the more general view of the first dimensions, the dimensions naturally become more unstable.

```{r}
#| label: fig-plot-coefficient-of-variation-comparison
#| fig-cap: "Eigenvalue stability plot for G-Gower and RelMS."
plot_cv_comparison
```

### Other comparisons

```{r}
create_comparison_plots("accommodates")
create_comparison_plots("host_total_listings_count")
create_comparison_plots("price")

create_comparison_plots("air_conditioning")
create_comparison_plots("heating")
create_comparison_plots("elevator")
create_comparison_plots("number_of_reviews")
```


## Clustering

After evaluating the available methods with the data, we chose not to use any of the hierarchical methods due to their poor performance. Instead, we compared PCA versus MDS using only non-hierarchical methods to evaluate their ability to cluster the data.

The average silhouette for each tested configuration can be seen in @fig-silhouette. We utilized PAM (Partitioning Around Medoids) and $k$-means as our clustering methods. For each method, we compared the performance of the Euclidean distance against the Mahalanobis distance.

:::{#fig-silhouette}

```{r}
plot(clustering_results$plots$silhouette)
```

Comparison of the silhouette of the different configurations.
:::

It is important to note that the average silhouette is not a robust metric in isolation; therefore, this plot serves only as a suggestion for which methods might perform better. Nevertheless, PCA with $k$-means and Mahalanobis distance appears to be the best approach for a small number of clusters, maintaining its performance as the number of clusters increases. For MDS, performance is initially lower but improves as the number of clusters increases.

Choosing a greater number of clusters comes at the cost of losing explicability, as it becomes more difficult to describe them. @fig-elbow shows the Within-Cluster Sum of Squares (WSS) for each configuration, a metric indicating how compact the points are within a cluster.

MDS produces a compact configuration with the Mahalanobis distance and PAM. This aligns with our previous analysis of MDS, where RelMS helped compact the data within clusters. However, the use of Euclidean distance significantly impacts this metric, as shown in the plot. The remaining configurations display similar performance.

:::{#fig-elbow}

```{r}
plot(clustering_results$plots$elbow)
```

Elbow method plot.
:::

@tbl-evaluation presents the final evaluation for each configuration with a computed score. This score is derived from the number of clusters, Silhouette width, WSS, Calinski-Harabasz index, and Dunn index. Higher values indicate better clustering performance.

:::{#tbl-evaluation}

| Space | Method  | Distance  |   k | Silhouette | CH_Index | Dunn_Index |     WSS |     Score |
| :---- | :------ | :-------- | --: | ---------: | -------: | ---------: | ------: | --------: |
| PCA   | K-means | Euclidean |   4 |     0.2535 |   264.88 |     0.0217 | 4199.12 | 0.7181471 |
| MDS   | K-means | Euclidean |  11 |     0.2627 |   137.29 |     0.1152 |  301.69 | 0.7130728 |
| PCA   | K-means | Euclidean |  11 |     0.2583 |   247.35 |     0.0251 | 2156.31 | 0.7089065 |
| PCA   | K-means | Euclidean |   5 |     0.2588 |   257.05 |     0.0217 | 3712.69 | 0.7043294 |
| PCA   | K-means | Euclidean |  10 |     0.2523 |   253.63 |     0.0251 | 2283.66 | 0.7017821 |
| PCA   | K-means | Euclidean |   3 |     0.2397 |   264.10 |     0.0168 | 4934.80 | 0.6946470 |
| MDS   | K-means | Euclidean |  12 |     0.2587 |   133.97 |     0.1178 |  289.17 | 0.6935491 |
| PCA   | K-means | Euclidean |   8 |     0.2434 |   258.87 |     0.0264 | 2670.66 | 0.6931150 |
| PCA   | K-means | Euclidean |   7 |     0.2418 |   262.51 |     0.0253 | 2919.07 | 0.6916564 |
| MDS   | K-means | Euclidean |   8 |     0.2381 |   141.20 |     0.1342 |  360.89 | 0.6870452 |
| MDS   | K-means | Euclidean |  10 |     0.2497 |   137.83 |     0.1127 |  319.78 | 0.6855834 |
| MDS   | K-means | Euclidean |   7 |     0.2298 |   146.92 |     0.1342 |  381.66 | 0.6808051 |
| PCA   | K-means | Euclidean |   6 |     0.2362 |   261.96 |     0.0191 | 3257.20 | 0.6792026 |
| PCA   | K-means | Euclidean |   9 |     0.2465 |   254.29 |     0.0137 | 2472.92 | 0.6751677 |
| MDS   | K-means | Euclidean |   9 |     0.2346 |   134.21 |     0.1342 |  345.82 | 0.6649749 |

Evaluation of the different configurations.
:::

Based on these results—and considering that a smaller number of clusters is easier to interpret—we selected the following two configurations for comparison:

1. PCA with $k$-means and Euclidean distance ($k=4$).
2. MDS with $k$-means and Euclidean distance ($k=8$).

Since the distance metric and clustering method are equivalent in both selected models, a fairer comparison can be attained.

:::{#fig-clusterings}

```{r}
plot(clustering_comparison)
```

Comparison between clustering PCA data against MDS data using $k$-means and the euclidean distance ($k=4$ against $k=8$).
:::

As demonstrated in @fig-clusterings, PCA produces a clearer clustering structure. PCA benefits from using only numerical data, avoiding the complexity of mixed categorical data. However, categorical data may reveal hidden patterns not visible in purely numerical approaches, representing a trade-off between explicability and completeness.
Plotting the data in three dimensions produces a clearer view on how the data is clustered (the added dimension allows for more flexibility and total variance explained).
Refer to @fig-pca-3d and @fig-mds-3d for more details on this.
The improvement is more significant on MDS than PCA.

:::{#fig-pca-3d}

```{r}
plot_pca_3d
```

:::

:::{#fig-mds-3d}

```{r}
plot_mds_3d
```

:::

:::{#tbl-pca}

```{r}
tabla_clusters_pca
```

Clustering characterization of the PCA data.
:::

:::{#tbl-mds}

```{r}
tabla_clusters_mds
```

Clustering characterization of the MDS data.
:::

We conclude this section by analyzing @tbl-pca and @tbl-mds. The key question is: are the distinct groups meaningful?

Using the PCA table as an example, we observe that the $p$-value for each variable is statistically significant. The test performed here to obtain it uses as null hypothesis that the median is identical across all groups (the alternative being, of course, that is not). This indicates significant differences between the groups for these variables.

- Cluster 2 contains hosts with an average of 317 listings (compared to the global median of 7). This suggests the group consists of corporate hosts or property management companies. Interestingly, this group is not distinguished by price, which follows the global trend.
- Cluster 4 has the highest value and is comprised almost entirely of "Entire homes/apartments".

In the case of MDS, explicability is more challenging due to the higher number of clusters ($k=8$). Nevertheless, the clustering is effective, as indicated by the significant $p$-values. The strongest differentiator between clusters is Room Type:

- Cluster 1: Exclusively Private Rooms (100%). It is also the cheapest cluster (Median Price: 51 €).
- Clusters 2, 3, and 7: Almost exclusively Entire Homes/Apartments (>98%), representing the premium segment.
  Clusters 4 and 5: "Mixed" types, suggesting these clusters are grouped by other factors (e.g., location) rather than just privacy.

Similar to the PCA results, Cluster 2 in the MDS analysis likely represents companies, as hosts here have a median of 27 listings (significantly higher than the overall median). This cluster also commands the highest price (154 €).

## Conclusions

MDS is a powerful technique when dealing with mixed-type data (numerical and categorical). While PCA is a well-known and widely applied technique, it lacks the flexibility to incorporate categorical variables directly.

However, MDS is often more difficult to explain due to a lack of direct variable association. In PCA, the principal components are linear combinations of the original variables (e.g., "size" or "price"), whereas in MDS, the dimensions represent relative distances, and there is no strict rule of thumb for interpretation.

Moreover, the dimensionality differs: PCA results in as many components as there are variables (columns), whereas MDS can theoretically produce as many dimensions as there are observations minus one (rows), though we typically select a low-dimensional representation.

Ultimately, the choice is a balance between efficiency and generality.

Clustering proves to be a powerful tool for revealing hidden patterns, as seen with the Airbnb listings. The results are significantly dependent on both the chosen distance metric and the clustering technique (recalling that the hierarchical approach was discarded due to poor performance). Additionally, clustering benefits from lower dimensionality, as fewer broad groups are often easier to interpret and operationalize than multiple small, fragmented clusters.
